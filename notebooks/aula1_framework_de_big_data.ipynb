{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark - Instalando a biblioteca PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `pip install pyspark` para instalar a biblioteca mais recente do PySpark no projeto.\n",
    "- `pip install findspark` para instalar a biblioteca mais recente do FindSpak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark # importa a biblioteca para o notebook\n",
    "findspark.init() #inicializa a biblioteca\n",
    "from pyspark.sql import SparkSession # essa função inicializa a sessão de uso do PySpark dentro do notebook\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               hello|\n",
      "+--------------------+\n",
      "|Sucesso total, es...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importa os dados para o projeto\n",
    "df = spark.sql('''select 'Sucesso total, estamos online!' as hello''')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalação  das principais bibliotecas do PySpark\n",
    "# referência para funções específicas para tratamento de dados\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType # \n",
    "from pyspark.sql.functions import col, expr, lit, substring, concat, concat_ws, when, coalesce\n",
    "from pyspark.sql import functions as F  # for more sql functions\n",
    "from functools import reduce\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.count:  551\n",
      "df.col ct:  7\n",
      "df.columns:  ['Bank Name', 'City', 'ST', 'CERT', 'Acquiring Institution', 'Closing Date', 'Updated Date']\n"
     ]
    }
   ],
   "source": [
    "# importa base de dados banklist\n",
    "# inferSchema - força a inferência do schema quando o arquivo for importado\n",
    "# header - informa o spark que há um cabeçalho no arquivo que está sendo importado\n",
    "\n",
    "url_github = 'https://raw.githubusercontent.com/camimq/big_data/main/bases/banklist.csv'\n",
    "\n",
    "pd_df = pd.read_csv(url_github)\n",
    "df_banklist = spark.createDataFrame(pd_df)\n",
    "# df_banklist = spark.read.csv(r'file_path_here', sep=',', inferSchema=True, header=True)\n",
    "\n",
    "print('df.count: ', df_banklist.count())\n",
    "print('df.col ct: ', len(df_banklist.columns))\n",
    "print('df.columns: ', df_banklist.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workaround para leitura de arquivo\n",
    "\n",
    "Para que eu não exponha o caminho de arquivo no meu computador, faço o _upload_ das bases no GitHub. Contudo, com o PySpark, por alguma razão, não consigo puxar o arquivo como fiz até o momento. Pesquisando na internet, descobri o _workaround_ acima no [StackOverflow](https://stackoverflow.com/questions/71251538/use-csv-from-github-in-pyspark) onde, basicamente, declaro uma variável com a URL onde o arquivo está e, posteriormente, crio outra variável de leitura do `csv` no pandas que, por último é utilizada para função `create` do Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SQL in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------+\n",
      "|           Bank Name|              City|Closing Date|\n",
      "+--------------------+------------------+------------+\n",
      "| Fayette County Bank|        Saint Elmo|   26-May-17|\n",
      "|Guaranty Bank, (d...|         Milwaukee|    5-May-17|\n",
      "|      First NBC Bank|       New Orleans|   28-Apr-17|\n",
      "|       Proficio Bank|Cottonwood Heights|    3-Mar-17|\n",
      "|Seaway Bank and T...|           Chicago|   27-Jan-17|\n",
      "|Harvest Community...|        Pennsville|   13-Jan-17|\n",
      "|         Allied Bank|          Mulberry|   23-Sep-16|\n",
      "|The Woodbury Bank...|          Woodbury|   19-Aug-16|\n",
      "|First CornerStone...|   King of Prussia|    6-May-16|\n",
      "|  Trust Company Bank|           Memphis|   29-Apr-16|\n",
      "|North Milwaukee S...|         Milwaukee|   11-Mar-16|\n",
      "|Hometown National...|          Longview|    2-Oct-15|\n",
      "| The Bank of Georgia|    Peachtree City|    2-Oct-15|\n",
      "|        Premier Bank|            Denver|   10-Jul-15|\n",
      "|      Edgebrook Bank|           Chicago|    8-May-15|\n",
      "|          Doral Bank|          San Juan|   27-Feb-15|\n",
      "|Capitol City Bank...|           Atlanta|   13-Feb-15|\n",
      "|Highland Communit...|           Chicago|   23-Jan-15|\n",
      "|First National Ba...|         Crestview|   16-Jan-15|\n",
      "|  Northern Star Bank|           Mankato|   19-Dec-14|\n",
      "+--------------------+------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_banklist.createOrReplaceTempView(\"banklist\")\n",
    "\n",
    "df_check = spark.sql('''select `Bank Name`, City, `Closing Date` from banklist''') # cria o dataframe utilizando um query SQL\n",
    "df_check.show()\n",
    "# df_check.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------+----+------------------+---------------------+------------+------------+\n",
      "|summary|           Bank Name|   City|  ST|              CERT|Acquiring Institution|Closing Date|Updated Date|\n",
      "+-------+--------------------+-------+----+------------------+---------------------+------------+------------+\n",
      "|  count|                 551|    551| 551|               551|                  551|         551|         551|\n",
      "|   mean|                NULL|   NULL|NULL|31729.392014519057|                 NULL|        NULL|        NULL|\n",
      "| stddev|                NULL|   NULL|NULL|16449.761310748272|                 NULL|        NULL|        NULL|\n",
      "|    min|1st American Stat...|Acworth|  AL|                91|      1st United Bank|    1-Aug-08|    1-Aug-13|\n",
      "|    max|               ebank|Wyoming|  WY|             58701|  Your Community Bank|    9-Sep-11|    9-Sep-12|\n",
      "+-------+--------------------+-------+----+------------------+---------------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_banklist.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----+\n",
      "|summary|   City|  ST|\n",
      "+-------+-------+----+\n",
      "|  count|    551| 551|\n",
      "|   mean|   NULL|NULL|\n",
      "| stddev|   NULL|NULL|\n",
      "|    min|Acworth|  AL|\n",
      "|    max|Wyoming|  WY|\n",
      "+-------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mostra a coluna Cidade e Estado\n",
    "df_banklist.describe('City', 'ST').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count, Columns and Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de linhas 551\n",
      "Total de colunas: 7\n",
      "Colunas: ['Bank Name', 'City', 'ST', 'CERT', 'Acquiring Institution', 'Closing Date', 'Updated Date']\n",
      "Tipo de dados: [('Bank Name', 'string'), ('City', 'string'), ('ST', 'string'), ('CERT', 'bigint'), ('Acquiring Institution', 'string'), ('Closing Date', 'string'), ('Updated Date', 'string')]\n",
      "Schema: StructType([StructField('Bank Name', StringType(), True), StructField('City', StringType(), True), StructField('ST', StringType(), True), StructField('CERT', LongType(), True), StructField('Acquiring Institution', StringType(), True), StructField('Closing Date', StringType(), True), StructField('Updated Date', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "print('Total de linhas', df_banklist.count())\n",
    "print('Total de colunas:', len(df_banklist.columns))\n",
    "print('Colunas:', df_banklist.columns)\n",
    "print('Tipo de dados:', df_banklist.dtypes)\n",
    "print('Schema:', df_banklist.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Bank Name: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- ST: string (nullable = true)\n",
      " |-- CERT: long (nullable = true)\n",
      " |-- Acquiring Institution: string (nullable = true)\n",
      " |-- Closing Date: string (nullable = true)\n",
      " |-- Updated Date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_banklist.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.count:  551\n",
      "df.columns:  ['Bank Name', 'City', 'ST', 'CERT', 'Acquiring Institution', 'Closing Date', 'Updated Date']\n"
     ]
    }
   ],
   "source": [
    "df_banklist = df_banklist.dropDuplicates()\n",
    "print('df.count: ', df_banklist.count())\n",
    "print('df.columns: ', df_banklist.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como não há mudança na contagem de linhas, é possível confirmar que não há dado duplicado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|           Bank Name|    City|\n",
      "+--------------------+--------+\n",
      "|         Allied Bank|Mulberry|\n",
      "|Highland Communit...| Chicago|\n",
      "+--------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df_banklist.select(*['Bank Name', 'City'])\n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------+------------+--------------------+--------+\n",
      "|Acquiring Institution|Closing Date|Updated Date|           Bank Name|    City|\n",
      "+---------------------+------------+------------+--------------------+--------+\n",
      "|         Today's Bank|   23-Sep-16|   17-Nov-16|         Allied Bank|Mulberry|\n",
      "| United Fidelity B...|   23-Jan-15|   21-Apr-15|Highland Communit...| Chicago|\n",
      "+---------------------+------------+------------+--------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# seleciona todas as colunas, exceto CERT e ST\n",
    "col_1 = list(set(df_banklist.columns) - {'CERT', 'ST'})\n",
    "df2 = df_banklist.select(*col_1)\n",
    "df2.show(2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-----+-----+--------------------+------------+------------+\n",
      "|           bank_name|           city|state| cert|     acq_institution|closing_date|updated_date|\n",
      "+--------------------+---------------+-----+-----+--------------------+------------+------------+\n",
      "|         Allied Bank|       Mulberry|   AR|   91|        Today's Bank|   23-Sep-16|   17-Nov-16|\n",
      "|Highland Communit...|        Chicago|   IL|20290|United Fidelity B...|   23-Jan-15|   21-Apr-15|\n",
      "|         Valley Bank|Fort Lauderdale|   FL|21793|Landmark Bank, Na...|   20-Jun-14|   29-Jun-15|\n",
      "|Heritage Bank of ...|    Orange Park|   FL|26680|  FirstAtlantic Bank|   19-Apr-13|    8-Aug-16|\n",
      "| Fayette County Bank|     Saint Elmo|   IL| 1802|United Fidelity B...|   26-May-17|    1-Jun-17|\n",
      "|       Covenant Bank|        Chicago|   IL|22476|Liberty Bank and ...|   15-Feb-13|   21-Sep-15|\n",
      "|  First Federal Bank|      Lexington|   KY|29594| Your Community Bank|   19-Apr-13|   12-Dec-16|\n",
      "|Hometown National...|       Longview|   WA|35156|      Twin City Bank|    2-Oct-15|   13-Apr-16|\n",
      "|Frontier Bank, FS...|    Palm Desert|   CA|34738|Bank of Southern ...|    7-Nov-14|   10-Nov-16|\n",
      "|The Community's Bank|     Bridgeport|   CT|57041|         No Acquirer|   13-Sep-13|    7-Dec-15|\n",
      "|      NBRS Financial|     Rising Sun|   MD| 4862|         Howard Bank|   17-Oct-14|   26-Mar-15|\n",
      "|Texas Community B...|  The Woodlands|   TX|57431|Spirit of Texas B...|   13-Dec-13|   29-Dec-14|\n",
      "|  Vantage Point Bank|        Horsham|   PA|58531|   First Choice Bank|   28-Feb-14|    3-Mar-15|\n",
      "|Pisgah Community ...|      Asheville|   NC|58701|  Capital Bank, N.A.|   10-May-13|    8-Aug-16|\n",
      "|  Trust Company Bank|        Memphis|   TN| 9956|The Bank of Fayet...|   29-Apr-16|    6-Sep-16|\n",
      "|Slavie Federal Sa...|        Bel Air|   MD|32368|       Bay Bank, FSB|   30-May-14|   12-Dec-16|\n",
      "|Guaranty Bank, (d...|      Milwaukee|   WI|30003|First-Citizens Ba...|    5-May-17|    1-Jun-17|\n",
      "|      Edgebrook Bank|        Chicago|   IL|57772|Republic Bank of ...|    8-May-15|   12-Jul-16|\n",
      "|Community Bank of...|  Sunrise Beach|   MO|27331|    Bank of Sullivan|   14-Dec-12|    4-Apr-14|\n",
      "|DuPage National Bank|   West Chicago|   IL| 5732|Republic Bank of ...|   17-Jan-14|   20-Oct-16|\n",
      "+--------------------+---------------+-----+-----+--------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# renomeando as colunas dentro do dataset\n",
    "df2 = df_banklist \\\n",
    "    .withColumnRenamed('Bank Name', 'bank_name')\\\n",
    "    .withColumnRenamed('Acquiring Institution', 'acq_institution')\\\n",
    "    .withColumnRenamed('Closing Date', 'closing_date')\\\n",
    "    .withColumnRenamed('ST', 'state')\\\n",
    "    .withColumnRenamed('CERT', 'cert')\\\n",
    "    .withColumnRenamed('City', 'city')\\\n",
    "    .withColumnRenamed('Updated Date', 'updated_date')#\\\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
